{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb48e198-920e-452a-a350-c9a9daed06e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "# Data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import bottleneck as bn\n",
    "import iris\n",
    "\n",
    "# Plotting\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91cb0823-a73d-4d69-808c-362f2b363162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some filepaths that will be used a lot\n",
    "home_dir = '/home/561/mg5624/RF_project/'\n",
    "my_data_dir = '/g/data/w97/mg5624/'\n",
    "shared_data_dir = '/g/data/w97/Shared_data/Observations/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2cc3aaf-bdd3-4b41-9bd1-063fc8475634",
   "metadata": {},
   "outputs": [],
   "source": [
    "COORDS = {\n",
    "    'SE_australia': {\n",
    "        'lats': (-38, -27),\n",
    "        'lons': (140, 154)\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ccd2cf-e5c9-450b-ad08-4a8540075f30",
   "metadata": {},
   "source": [
    "## Take Necessary Data from Sanaa's Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94177663-1080-44e0-9c01-a8de4e48889f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Year     Month Year_Month     Location  Latitude  Longitude  Drought  \\\n",
      "0    2014     March    2014-03        Cobar  -31.4980   145.8383        1   \n",
      "1    2014  February    2014-02        Cobar  -31.4980   145.8383        1   \n",
      "2    2014   January    2014-01        Cobar  -31.4980   145.8383        1   \n",
      "3    2014     March    2014-03      Walgett  -30.0167   148.1167        1   \n",
      "4    2014  February    2014-02      Walgett  -30.0167   148.1167        1   \n",
      "..    ...       ...        ...          ...       ...        ...      ...   \n",
      "930  2020  November    2020-11  Wagga Wagga  -35.1330   147.3670        0   \n",
      "931  2020  December    2020-12  Wagga Wagga  -35.1330   147.3670        0   \n",
      "932  2021   January    2021-01  Wagga Wagga  -35.1330   147.3670        0   \n",
      "933  2021  February    2021-02  Wagga Wagga  -35.1330   147.3670        0   \n",
      "934  2021     March    2021-03  Wagga Wagga  -35.1330   147.3670        0   \n",
      "\n",
      "     Deep_Drainage  PET_Actual    E_Actual  Soil_M_root_zone      Qtot  \\\n",
      "0         2.349609  157.679688   70.867188          0.265625  0.500000   \n",
      "1         1.896484  203.679688   28.414062          0.146484  2.351562   \n",
      "2         2.130859  273.570312   30.031250          0.082031  0.078125   \n",
      "3         0.298828  176.757812   27.703125          0.082031  0.054688   \n",
      "4         0.271484  203.789062    7.820312          0.023438  0.015625   \n",
      "..             ...         ...         ...               ...       ...   \n",
      "930       1.437500  194.179688   79.210938          0.224609  0.054688   \n",
      "931       1.455078  215.671875   71.375000          0.196289  0.148438   \n",
      "932       1.431641  209.195312   57.593750          0.111328  0.210938   \n",
      "933       1.328125  167.609375  100.804688          0.370117  0.929688   \n",
      "934       1.493164  126.960938   73.601562          0.190430  0.218750   \n",
      "\n",
      "       Rainfall  ENSO    IOD   SAM    P_acc_3M  \n",
      "0     57.000000  0.54 -0.051  1.34  145.500000  \n",
      "1     84.101562 -0.37 -0.009  0.36  119.500000  \n",
      "2      4.398438 -1.26 -0.036 -0.13   37.000000  \n",
      "3     34.000000  0.54 -0.051  1.34   68.203125  \n",
      "4     27.000000 -0.37 -0.009  0.36   41.000000  \n",
      "..          ...   ...    ...   ...         ...  \n",
      "930   37.398438 -1.19  0.143  1.14  194.304688  \n",
      "931   63.500000 -1.86  0.100  2.28  199.906250  \n",
      "932   76.796875 -1.87  0.116  1.95  173.601562  \n",
      "933   91.601562 -1.84  0.323  2.19  225.796875  \n",
      "934  103.203125 -0.55  0.367 -0.95  271.390625  \n",
      "\n",
      "[935 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load in Database from Sanaa's RF model\n",
    "df = pd.read_csv(home_dir + '/Data/ML_Database_All_AWRA_MOf_and_3MPrecip.csv')\n",
    "\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "\n",
    "df.rename(columns = {'Drought / No Drought': 'Drought'}, inplace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "269d3726-b0d8-48d3-8638-a0d85cfec04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two databases from it - one with just the drought/no drought colomn and one with the climate drivers\n",
    "columns_to_drop_for_drought = ['Deep_Drainage', 'PET_Actual', 'E_Actual', 'Soil_M_root_zone', 'Qtot', 'Rainfall', 'ENSO', 'IOD', 'SAM', 'P_acc_3M']\n",
    "training_df = df.drop(columns_to_drop_for_drought, axis=1)\n",
    "\n",
    "training_df.to_csv(home_dir + '/Data/drought_dataframe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca277eff-5bae-4ad1-beea-a59dbf01b311",
   "metadata": {},
   "source": [
    "# Process New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b83357bb-eaaa-41d5-b853-aa0f67625835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions needed for processing training data\n",
    "def add_predictor_to_training_df(training_dataframe, predictor_dataset, predictor_name, replace=False):\n",
    "    \"\"\"\n",
    "    Function which adds relevant data to the training dataframe.\n",
    "    \n",
    "    Args:\n",
    "        training_dataframe (pd.DataFrame): dataframe containing all other training data\n",
    "        predictor_dataset (xr.DataArray): xarray dataset of the predictor variable\n",
    "        predictor_name (str): full name of the predictor variable\n",
    "        replace (bool): if True and predictor_name already in df, it is replaced with new one (default=False)\n",
    "    \"\"\"\n",
    "    # Return the original dataframe if predictor_name exists and we're not replacing it.\n",
    "    if predictor_name in training_dataframe and not replace:\n",
    "        return training_dataframe\n",
    "\n",
    "    # Add empty column to training_df of predictor variable\n",
    "    training_dataframe[predictor_name] = None\n",
    "    \n",
    "    # Find the lat, long, and time columns from the dataframe\n",
    "    training_longs = training_dataframe['Longitude']\n",
    "    training_lats = training_dataframe['Latitude']\n",
    "    training_time = training_dataframe['Year_Month']\n",
    "\n",
    "    # Loop over each entry in training_df and add the corresponding entry from the predictor dataset\n",
    "    for i, time in enumerate(training_time):\n",
    "        time_check = pd.to_datetime(time)\n",
    "        if time_check <= predictor_dataset.coords['time'][-1].data:\n",
    "            predictor_at_i = predictor_dataset.sel(time=time).sel(lon=training_longs[i], lat=training_lats[i], method='nearest').data\n",
    "            training_dataframe[predictor_name].iat[i] = predictor_at_i\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return training_dataframe\n",
    "            \n",
    "\n",
    "def rename_coord_titles_to_lat_long(dataset):\n",
    "    \"\"\"\n",
    "    Changes the titles of the coordinates to lat long to keep it consistent\n",
    "    \n",
    "    Args:\n",
    "        dataset (xr.DataSet): dataset with incorrect coordinate titles\n",
    "    \"\"\"\n",
    "    # Define mapping from old to new name\n",
    "\n",
    "    mapping = {\n",
    "        'X': 'lon',\n",
    "        'Y': 'lat'\n",
    "    }\n",
    "    \n",
    "    renamed_dataset = dataset.rename(mapping)\n",
    "\n",
    "    return renamed_dataset\n",
    "\n",
    "\n",
    "def add_year_month_coord_to_dataarray(dataarray):\n",
    "    \"\"\"\n",
    "    Adds year, month, and year_month coordinates to the input dataarray. \n",
    "    Requires dataarray to have time cooridnate in datetime format.\n",
    "\n",
    "    Args:\n",
    "        dataarray (xr.DataArray): dataarray to add coordinates to\n",
    "\n",
    "    Returns:\n",
    "        dataarray (xr.DataArray): \n",
    "            dataarray with additonal year, month, and year_month coordinates\n",
    "    \"\"\"\n",
    "    # Add year_month coordinates to data\n",
    "    dataarray['Year'] = dataarray['time'].dt.strftime('%Y')\n",
    "    dataarray['Month'] = dataarray['time'].dt.strftime('%m')\n",
    "    dataarray['Year_Month'] = dataarray['Year'] + '-' + dataarray['Month']\n",
    "\n",
    "    return dataarray\n",
    "\n",
    "\n",
    "\n",
    "def create_predictor_dataframe(start_year, end_year, area, data, data_name):\n",
    "    \"\"\"\n",
    "    Creates a dataframe that runs from start year to end year, to be used to\n",
    "    predict droughts in that time period. Predictors can then be added to \n",
    "    this dataframe using the add_predictor_to_predictors_dataframe function.\n",
    "    \n",
    "    Args:\n",
    "        start_year (int): start year\n",
    "        end_year (int): end year\n",
    "        area (str): one of the areas defined in the COORDS dictionary\n",
    "        data (xr.DataSet or xr.DataArray): \n",
    "            data of a predictor containing at least data covering the time period and area of interest\n",
    "        data_name (str): name of the data\n",
    "    \n",
    "    Returns:\n",
    "        constrained_df (pd.DataFrame): dataframe of data over specified area and time \n",
    "    \"\"\"\n",
    "    data = data.rename(data_name)\n",
    "    \n",
    "    lats = COORDS[area]['lats']\n",
    "    lons = COORDS[area]['lons']\n",
    "\n",
    "    lat_min, lat_max = lats[0], lats[1]\n",
    "    lon_min, lon_max = lons[0], lons[1]\n",
    "\n",
    "    start_year = str(start_year)\n",
    "    end_year = str(end_year)\n",
    "    \n",
    "    constrained_data = data.sel(\n",
    "        time=slice(start_year, end_year),\n",
    "        lat=slice(lat_min, lat_max),\n",
    "        lon=slice(lon_min, lon_max)\n",
    "    )\n",
    "\n",
    "    constrained_data = add_year_month_coord_to_dataarray(constrained_data)\n",
    "\n",
    "    constrained_df = constrained_data.to_dataframe()\n",
    "    constrained_df.reset_index(inplace=True)\n",
    "\n",
    "    coord_rename = {\n",
    "        'lon': 'Longitude',\n",
    "        'lat': 'Latitude'\n",
    "    }\n",
    "    \n",
    "    constrained_df.rename(columns=coord_rename, inplace=True)\n",
    "   \n",
    "    constrained_df['Latitude'] = constrained_df['Latitude'].astype(float).round(2)\n",
    "    constrained_df['Longitude'] = constrained_df['Longitude'].astype(float).round(2)\n",
    "    \n",
    "    return constrained_df\n",
    "\n",
    "\n",
    "def add_predictor_to_predictors_dataframe(predictors_df, new_predictor_data, new_predictor_name, replace=False):\n",
    "    \"\"\"\n",
    "    Adds specified predictor data into the predictors dataframe.\n",
    "\n",
    "    Args:\n",
    "        predictor_df (pd.DataFrame): dataframe of the predictors\n",
    "        new_predictor_data (xr.DataArray or pd.DataFrame): new predictor data to be added\n",
    "        new_predictor_name (str): name of the new predictor\n",
    "        replace (bool): if true, new predictor will replace any of the same name in dataframe\n",
    "\n",
    "    Returns:\n",
    "        merged_df (pd.DataFrame): predicotr_df with additional predictor in it            \n",
    "    \"\"\"\n",
    "    # Return the original dataframe if predictor_name exists and we're not replacing it.\n",
    "    if new_predictor_name in predictors_df and not replace:\n",
    "        return predictors_df\n",
    "    \n",
    "    if isinstance(new_predictor_data, (xr.DataArray, xr.Dataset)):\n",
    "        new_predictor_data = add_year_month_coord_to_dataarray(new_predictor_data)\n",
    "        new_predictor_df = new_predictor_data.to_dataframe()\n",
    "        new_predictor_df.reset_index(inplace=True)\n",
    "    else:\n",
    "        new_predictor_df = new_predictor_data\n",
    "        dataframe['Month'] = dataframe['Month'].astype(int).astype(str)\n",
    "        dataframe[\"Year_Month\"] = dataframe['Year'].astype(str) + '-' + dataframe['Month'].str.zfill(2)\n",
    "    \n",
    "    merged_df = pd.merge(predictors_df, new_predictor_df, on='Year_Month', how='inner')\n",
    "\n",
    "    return merged_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcbf560-98be-4760-88cf-872df37899fe",
   "metadata": {},
   "source": [
    "## Precipitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677ac3f0-f208-45e8-b6b8-56a90674517e",
   "metadata": {},
   "source": [
    "Using AGCD precipitation data which runs from 1900 to 2023 across the whole of Australia. Resolution is at 0.05 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00e11cbd-8c6d-4860-b71e-681d38015af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'Precipitation' (time: 1464, lat: 691, lon: 861)>\n",
      "[871008264 values with dtype=float32]\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 1900-01-16 1900-02-14 ... 2021-12-16\n",
      "  * lon      (lon) float32 112.0 112.1 112.1 112.2 ... 154.9 154.9 154.9 155.0\n",
      "  * lat      (lat) float32 -44.5 -44.45 -44.4 -44.35 ... -10.1 -10.05 -10.0\n",
      "Attributes:\n",
      "    standard_name:              lwe_thickness_of_precipitation_amount\n",
      "    long_name:                  Daily precipitation\n",
      "    units:                      mm\n",
      "    grid_mapping:               crs\n",
      "    cell_methods:               time: sum time: sum (interval : 1 day)\n",
      "    analysis_version_number:    3.01\n",
      "    start_offset:               -24\n",
      "    frequency:                  monthly\n",
      "    length_scale_for_analysis:  250.0\n",
      "    coverage_content_type:      physicalMeasurement\n"
     ]
    }
   ],
   "source": [
    "precip_filepath = my_data_dir + '/RF_project/Precipitation/AGCD/'\n",
    "precip = xr.open_dataarray(precip_filepath + 'AGCD_v1_precip_total_r005_monthly_1900_2021.nc')\n",
    "print(precip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ee659a5-50b6-41a3-bdff-9a9f1094f1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "filepath defined\n",
      "precip loaded\n",
      "added precip, lagged precip to df\n",
      "3\n",
      "6\n",
      "12\n",
      "24\n",
      "36\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "print('starting')\n",
    "precip_filepath = my_data_dir + '/RF_project/Precipitation/AGCD/'\n",
    "print('filepath defined')\n",
    "precip = xr.open_dataarray(precip_filepath + 'AGCD_v1_precip_total_r005_monthly_1900_2021.nc')\n",
    "# precip = precip_ds.precip\n",
    "print('precip loaded')\n",
    "# previous_precip = precip.shift(time=1)\n",
    "# print('prev precip calculated')\n",
    "\n",
    "# training_df = add_predictor_to_training_df(training_df, previous_precip, 'Lagged_Precipitation')\n",
    "training_df = add_predictor_to_training_df(training_df, precip, 'Precipitation')\n",
    "print('added precip, lagged precip to df')\n",
    "n_months = [3, 6, 12, 24, 36, 48]\n",
    "\n",
    "for n in n_months:\n",
    "    print(n)\n",
    "    n_monthly_precip = xr.open_dataarray(precip_filepath + f'AGCD_v1_precip_total_r005_{n}monthly_1900_2021.nc')\n",
    "    training_df = add_predictor_to_training_df(training_df, n_monthly_precip, f'Acc_{n}-Month_Precipitation')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8153d533-5396-45a3-b3c6-6681cb384f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictors_df_1980 = add_predictor_to_predictors_dataframe(predictors_df_1980, precip_3months, 'Acc_3-Month_Precipitation')\n",
    "# print(predictors_df_1980)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5a4c14-4998-429f-8d8f-4851ca55ebd4",
   "metadata": {},
   "source": [
    "## Runoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c7e4820-c018-44ee-81ce-c3708917e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "runoff = xr.open_dataarray('/g/data/w97/mg5624/RF_project/Runoff/AWRA/AWRAv7_Runoff_month_1911_2023.nc')\n",
    "training_df = add_predictor_to_training_df(training_df, runoff, 'Runoff', replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b143148-38de-42f5-b014-92dd4fec6fb4",
   "metadata": {},
   "source": [
    "## Climate Drivers: ENSO, IOD, SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62ab4122-957c-4ed0-9142-ffeb46947f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_tabled_dataframe_into_correct_form(dataframe, driver):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe with years down column 1 and months along row 1 and sorts it so that\n",
    "    the columns are \"Year\", \"Month\", \"Year_Month\", index.\n",
    "    \n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): Dataframe of the dirvers index in tabular form\n",
    "        driver (str): name of the driver\n",
    "    \"\"\"    \n",
    "    original_cols = dataframe.columns\n",
    "    if isinstance(original_cols[1], int) or original_cols[1] == '1':\n",
    "        # Transform dataframe so that it has a month column instead of months on the rows\n",
    "        melted_df = dataframe.melt(id_vars=['Year'], var_name='Month', value_name=f'{driver}_index')\n",
    "        dataframe = melted_df\n",
    "    else:    \n",
    "        dataframe.rename(columns={'Index': f'{driver}_index'}, inplace=True)\n",
    "\n",
    "    # Ensure Months are saved as 1 digit strings\n",
    "    dataframe['Month'] = dataframe['Month'].astype(int).astype(str)\n",
    "    \n",
    "    # Create new \"Year_Month\" column\n",
    "    dataframe[\"Year_Month\"] = dataframe['Year'].astype(str) + '-' + dataframe['Month'].str.zfill(2)\n",
    "\n",
    "    # Swap \"Year_Month\" column with index column\n",
    "    new_cols = list(dataframe.columns)\n",
    "    new_cols[3], new_cols[2] = new_cols[2], new_cols[3]\n",
    "    switch_cols_df = dataframe[new_cols]\n",
    "\n",
    "    # Remove Year and Month columns\n",
    "    final_df = switch_cols_df[['Year_Month', f'{driver}_index']]\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def add_drivers_df_to_training_df(training_df, driver):\n",
    "    \"\"\"\n",
    "    Adds the data from the drivers index to the training dataframe.\n",
    "    \n",
    "    Args:\n",
    "        training_df (pd.DataFrame): Dataframe containing the training data\n",
    "        driver (str): name of the driver\n",
    "    \"\"\"\n",
    "    # Define dictionary of driver to index name\n",
    "    index_name = {'ENSO': 'BEST', 'IOD': 'DMI', 'SAM': 'AAO'}\n",
    "    \n",
    "    # Sort the dataframe into correct format\n",
    "    df = pd.read_csv(my_data_dir + f'RF_project/{driver}/{driver}_{index_name[driver]}_index.csv')\n",
    "\n",
    "    sorted_df = sort_tabled_dataframe_into_correct_form(df, driver)\n",
    "    \n",
    "    # Merge driver dataframe into training dataframe\n",
    "    merged_df = pd.merge(training_df, sorted_df, on='Year_Month', how='inner')\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56406f85-bac7-4dce-8339-fa581b6361ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "drivers = ['ENSO', 'IOD', 'SAM']\n",
    "index_name = {'ENSO': 'BEST', 'IOD': 'DMI', 'SAM': 'AAO'}\n",
    "\n",
    "for driver in drivers:    \n",
    "    # Add the drivers to the training dataframe if they're not in there yet\n",
    "    if not f'{driver}_index' in training_df.columns:\n",
    "        training_df = add_drivers_df_to_training_df(training_df, driver)\n",
    "\n",
    "    # Save the full drivers dataframes to my data dir\n",
    "    df = pd.read_csv(my_data_dir + f'RF_project/{driver}/{driver}_{index_name[driver]}_index.csv')\n",
    "    sorted_df = sort_tabled_dataframe_into_correct_form(df, driver)\n",
    "    sorted_df.to_csv(my_data_dir + f'RF_project/{driver}/{driver}_{index_name[driver]}_index_sorted.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52892641-f2d0-4f36-8467-e14876de65b8",
   "metadata": {},
   "source": [
    "## Evapotranspiration and Potential Evapotranspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b0609cb-0aa9-45d8-b090-cda2935aff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLEAM_data_path = my_data_dir + f'RF_project/ET_products/v3_6/'\n",
    "\n",
    "\n",
    "ET = xr.open_dataarray(GLEAM_data_path + 'ET/ET_1980-2021_GLEAM_v3.6a_MO_Australia_0.05grid.nc')\n",
    "PET = xr.open_dataarray(GLEAM_data_path + 'PET/PET_1980-2021_GLEAM_v3.6a_MO_Australia_0.05grid.nc')\n",
    "\n",
    "training_df = add_predictor_to_training_df(training_df, ET, 'ET', replace=True)\n",
    "training_df = add_predictor_to_training_df(training_df, PET, 'PET', replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd5d827-9740-4dbf-9e2f-efa751673978",
   "metadata": {},
   "source": [
    "## Soil Moisture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02b7b0e7-ee6e-42c3-93cb-eb8b315b16bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (time: 516, lon: 861, lat: 691)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 1980-01-31 1980-02-29 ... 2022-12-31\n",
      "  * lon      (lon) float32 112.0 112.1 112.1 112.2 ... 154.9 154.9 154.9 155.0\n",
      "  * lat      (lat) float32 -44.5 -44.45 -44.4 -44.35 ... -10.1 -10.05 -10.0\n",
      "Data variables:\n",
      "    SMsurf   (time, lat, lon) float32 ...\n",
      "<xarray.Dataset>\n",
      "Dimensions:  (time: 516, lon: 861, lat: 691)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 1980-01-31 1980-02-29 ... 2022-12-31\n",
      "  * lon      (lon) float32 112.0 112.1 112.1 112.2 ... 154.9 154.9 154.9 155.0\n",
      "  * lat      (lat) float32 -44.5 -44.45 -44.4 -44.35 ... -10.1 -10.05 -10.0\n",
      "Data variables:\n",
      "    SMroot   (time, lat, lon) float32 ...\n"
     ]
    }
   ],
   "source": [
    "SM_path = my_data_dir + 'RF_project/Soil_Moisture/v3_8/'\n",
    "SM_vars = ['SMsurf', 'SMroot']\n",
    "\n",
    "for var in SM_vars:\n",
    "    sm_dataset = xr.open_dataset(SM_path + f'{var}/{var}_1980-2022_GLEAM_v3.8a_MO_Australia_0.05grid.nc')\n",
    "    print(sm_dataset)\n",
    "    if var == 'SMsurf':\n",
    "        sm_dataarray = sm_dataset.SMsurf\n",
    "    else:\n",
    "        sm_dataarray = sm_dataset.SMroot\n",
    " \n",
    "    training_df = add_predictor_to_training_df(training_df, sm_dataarray, var, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ab4797-2d27-4815-9bf3-7f6e33959b56",
   "metadata": {},
   "source": [
    "## Change in Water Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e4befea-5c1e-43ac-9a4e-70ddaa9e0653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = '/g/data/w97/mg5624/RF_project/Water_Storage/GTWS-MLrec/CSR-based_GTWS-MLrec_CWS_australia_0.05_grid.nc'\n",
    "# CWS = xr.open_dataarray(file)\n",
    "# training_df = add_predictor_to_training_df(training_df, CWS, \"CWS\")\n",
    "# training_df.replace(np.nan, pd.NA, inplace=True)\n",
    "# # print(training_df)\n",
    "# training_df.dropna(axis=0, inplace=True)\n",
    "# nans = training_df['CWS'].isna()\n",
    "# print(training_df['CWS'][0], type(training_df['CWS'][0]))\n",
    "# # print(point)\n",
    "# print(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b287b19-eb85-4578-a7fe-4a5a9f57aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0fb6c7-302b-41ae-913f-bea78d236a4e",
   "metadata": {},
   "source": [
    "## Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5375daae-429e-42b8-9b23-28a2c324367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cyclical_month_columns_to_training_df(training_dataframe):\n",
    "    \"\"\"\n",
    "    Adds two extra columns to training dataframe (sine_month and cosine_month) to proved cyclical months.\n",
    "    \n",
    "    Args:\n",
    "        training_dataframe (pd.DataFrame): Dataframe containing the training data\n",
    "    \"\"\"\n",
    "    months  = [\n",
    "        'January', 'February', 'March', 'April', 'May', 'June', \n",
    "        'July', 'August', 'September', 'October', 'November', 'December'\n",
    "    ]\n",
    "\n",
    "    month_numbers = np.arange(1, 13)\n",
    "\n",
    "    angles = 2 * np.pi * month_numbers / 12\n",
    "    sin_month = np.sin(angles)\n",
    "    cos_month = np.cos(angles)\n",
    "\n",
    "    month_data = {'Month': months, 'Sin_month': sin_month, 'Cos_month': cos_month}\n",
    "    month_df = pd.DataFrame(month_data)\n",
    "    \n",
    "    merged_df = pd.merge(training_dataframe, month_df, on='Month', how='inner')\n",
    "\n",
    "    return merged_df\n",
    "    \n",
    "\n",
    "training_df = add_cyclical_month_columns_to_training_df(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb89fb-9b97-4cd0-bb09-44eb0361b69a",
   "metadata": {},
   "source": [
    "## NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de3a0637-8ee3-4c84-974d-8f965ded9434",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/g/data/w97/mg5624/RF_project/NDVI/australia/'\n",
    "filename = 'ndvi3g_geo_v1_1_1982-2022.nc'\n",
    "NDVI = xr.open_dataarray(filepath + filename)\n",
    "\n",
    "NDVI = xr.where((NDVI >= -0.3) | (NDVI <= 1), NDVI, np.nan)\n",
    "\n",
    "# print(NDVI)\n",
    "# NDVI_no_nan = NDVI.dropna(dim='time').dropna(dim='lat').dropna(dim='lon')\n",
    "# lower_invalid = NDVI < -0.3\n",
    "# upper_invalid= NDVI > 1\n",
    "\n",
    "# invalid_values = NDVI_no_nan.where(lower_invalid | upper_invalid, drop=True)\n",
    "\n",
    "# # print(invalid_values)\n",
    "\n",
    "# print('max:', NDVI.max().item())\n",
    "# print('min:', NDVI.min().item())\n",
    "# # NDVI_lower_invalid = NDVI.where(~np.isnan(NDVI) < -0.3)\n",
    "# print(np.nanmax(NDVI_lower_invalid.data), np.nanmin(NDVI_lower_invalid.data))\n",
    "# NDVI_lower_invalid = ~np.isnan(NDVI_lower_invalid)\n",
    "# print('NDVI points below -0.3:', NDVI_lower_invalid.data[NDVI_lower_invalid.data==True])\n",
    "\n",
    "# NDVI_upper_invalid = NDVI.where(NDVI > 1, drop=True)\n",
    "# NDVI_upper_invalid = ~np.isnan(NDVI_upper_invalid)\n",
    "# print('NDVI points above 1:', NDVI_upper_invalid.data[NDVI_upper_invalid.data==True])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d18875-e123-4e65-8ef7-b27f4cd03d5e",
   "metadata": {},
   "source": [
    "## Save Training Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cc1f798-8abd-4022-aae3-9f229e23c88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Year', 'Month', 'Year_Month', 'Location', 'Latitude', 'Longitude',\n",
      "       'Drought', 'Precipitation', 'Acc_3-Month_Precipitation',\n",
      "       'Acc_6-Month_Precipitation', 'Acc_12-Month_Precipitation',\n",
      "       'Acc_24-Month_Precipitation', 'Acc_36-Month_Precipitation',\n",
      "       'Acc_48-Month_Precipitation', 'Runoff', 'ENSO_index', 'IOD_index',\n",
      "       'SAM_index', 'ET', 'PET', 'SMsurf', 'SMroot', 'Sin_month', 'Cos_month'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "training_df.to_csv(my_data_dir + f'RF_project/training_data/training_data.csv')\n",
    "print(training_df.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3]",
   "language": "python",
   "name": "conda-env-analysis3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
